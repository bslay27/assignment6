# -*- coding: utf-8 -*-
"""assignment6

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3UBC9F8EEziuQZGcXnDeZzC4klk9noE

### **Problem 1a.** Develop a Fully Connected Neural Network with only one hidden layer (size of 32) to predict the housing value for the housing dataset. Make sure to include all input features. Compare your training loss value and validation results against the linear regression you implemented in Assignment 5. Can you compare your model complexity (number of trainable parameters) against linear regression?
"""

# Importing the dataset

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/ECGR 4105/Assignment2/Housing.csv'
housing_dataset = pd.read_csv(file_path)
housing_dataset.head()

'''

Data Preprocessing

'''
X_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
y_vars = 'price'

X = housing_dataset[X_vars]

# Changing 'yes' and 'no' to 1 and 0
X = pd.get_dummies(X, drop_first=True)
y = housing_dataset[y_vars]

from sklearn.model_selection import train_test_split
# Performing 20% and 80% split between training and evaluation (test)
X_training, X_test, y_training, y_test = train_test_split(X, y, test_size = 0.8, random_state = 0)

'''

Standardizing Input and Output values

'''
from sklearn.preprocessing import StandardScaler
X_scaler = StandardScaler()
X_training = X_scaler.fit_transform(X_training)
X_test = X_scaler.transform(X_test)

Y_scaler = StandardScaler()
y_training = Y_scaler.fit_transform(y_training.values.reshape(-1, 1)).flatten()
y_test = Y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

'''

Input and Output Values as Pytorch Tensors

'''

import torch
X_training = torch.tensor(X_training, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_training = torch.tensor(y_training, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)


'''

Defining Neural Network Model with 1 hidden layer

'''

import torch.nn as nn
import torch.optim as optim
# Model with size of 32 - one hidden layer
nnModel = nn.Sequential(
    nn.Linear(11, 32),
    nn.Tanh(),
    nn.Linear(32, 1)
)

epochs = 5000
learning_Rate = 0.001
loss_fn = nn.MSELoss()
optimizer = optim.SGD(nnModel.parameters(), lr = learning_Rate)

'''

Training Loop Function

'''

def training_Loop(n_epochs, optimizer, model, loss_fn, X_training, X_test, y_training, y_test):
  for epoch in range(1, n_epochs + 1):
    model.train()
    t_p_training = model(X_training)
    loss_training = loss_fn(t_p_training, y_training)
    model.eval()
    with torch.no_grad():
      t_p_test = model(X_test)
      loss_test = loss_fn(t_p_test, y_test)

    optimizer.zero_grad()
    loss_training.backward()
    optimizer.step()
    if epoch == 1 or epoch % 500 == 0:
        print(f'Epoch {epoch}, Training Loss {loss_training.item():.4f}, Validation Loss {loss_test.item():.4f}')

'''

Executing Training Loop with Neural Network Model

'''

training_Loop(
    n_epochs = epochs,
    optimizer = optimizer,
    model = nnModel,
    loss_fn = loss_fn,
    X_training = X_training,
    X_test = X_test,
    y_training = y_training,
    y_test = y_test
)

"""### **Problem 1b.** We will increase the network complexity by adding two additional hidden layers, the hidden layers overall. My suggestions for the size of layers are: 32, 64, and 16 respectively. Please redesign the network and compare your training loss value and validation results against the linear regression you implemented in Assignment 5 and Problem 1.a. Can you compare your model complexity? Note: Use the same 20%, and 80% split for training and validation."""

'''

Defining Neural Network Model with 3 hidden layers

'''
import torch.nn as nn
import torch.optim as optim
# Model
nnModel = nn.Sequential(
    nn.Linear(11, 32),
    nn.Tanh(),
    nn.Linear(32, 64),
    nn.Tanh(),
    nn.Linear(64, 16),
    nn.Tanh(),
    nn.Linear(16, 1)
)

epochs = 5000
learning_Rate = 0.001
loss_fn = nn.MSELoss()
optimizer = optim.SGD(nnModel.parameters(), lr = learning_Rate)

'''

Executing Training Loop with Neural Network Model

'''

training_Loop(
    n_epochs = epochs,
    optimizer = optimizer,
    model = nnModel,
    loss_fn = loss_fn,
    X_training = X_training,
    X_test = X_test,
    y_training = y_training,
    y_test = y_test
)

"""### **Problem 2a.** Create a fully connected Neural Network for all 10 classes in CIFAR-10 with only one hidden layer with a size of 512. Report your training time, training loss, and evaluation accuracy. Analyze your results in your report.

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import time

cifar10_training_dataset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transforms.ToTensor())
cifar10_test_dataset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transforms.ToTensor())
training_loader = torch.utils.data.DataLoader(cifar10_training_dataset, batch_size = 64, shuffle = True)
test_loader = torch.utils.data.DataLoader(cifar10_test_dataset, batch_size = 64, shuffle = False)


'''

Defining Neural Network Model with 1 hidden layer

'''

epochs = 300
learning_Rate = 0.001
nnModel_cifar10 = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 10),
)

loss_fn_cifar10 = nn.CrossEntropyLoss()
optimizer = optim.SGD(nnModel_cifar10.parameters(), lr = learning_Rate)

'''

Training Loop Function

'''

def training_Loop (n_epochs, optimizer, model, loss_fn, training_loader):
    starting_time = 0
    ending_time = 0
    starting_time = time.time()
    for epoch in range(1, n_epochs + 1):
        running_loss = 0.0
        for images, labels in training_loader:
          optimizer.zero_grad()
          images = images.view(images.shape[0], -1)
          outputs = model(images)
          loss_training = loss_fn_cifar10(outputs, labels)
          loss_training.backward()
          optimizer.step()
          running_loss += loss_training.item()
        ending_time = time.time()
        if epoch == 1 or epoch % 25 == 0:
          print(f'Epoch {epoch}, Training Loss {running_loss/len(training_loader):.4f}, Training Time {ending_time - starting_time:.4f} seconds')
    ending_time = time.time()
    print(f'Total Training Time: {ending_time - starting_time:.4f} seconds')

    # Testing/Validation - Model Evaluation

    correctEval = 0
    totalEval = 0
    starting_time_test = time.time()
    with torch.no_grad():
      for images, labels in test_loader:
        images = images.view(images.shape[0], -1)
        outputs = nnModel_cifar10(images)
        _, predicted = torch.max(outputs.data, 1)
        totalEval += labels.size(0)
        correctEval += (predicted == labels).sum().item()
    ending_time_test = time.time()
    totalAccuracy = (correctEval / totalEval) *100
    print(f'Accuracy of Neural Network : {totalAccuracy}%')
    print(f'Total Validation Time: {ending_time_test - starting_time_test:.4f} seconds')


'''

Executing Training Loop with Neural Network Model

'''

training_Loop(
    n_epochs = epochs,
    optimizer = optimizer,
    model = nnModel_cifar10,
    loss_fn = loss_fn_cifar10,
    training_loader = training_loader
)

"""### **Problem 2b.** Extend your network with two more additional hidden layers, like the example we did in the lecture (pick the sizes of hidden layers properly). Train your network. Report your training time, loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem 2.a. Do you see any over-fitting? Can you compare your model complexity against problem 2.a? (35pt)


"""

'''

Defining Neural Network Model with 3 hidden layers

'''
# Model with size of 32 - one hidden layer
nnModel_cifar10 = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 1024),
    nn.Tanh(),
    nn.Linear(1024, 256),
    nn.Tanh(),
    nn.Linear(256, 10)
)

epochs = 300
learning_Rate = 0.001

loss_fn_cifar10 = nn.CrossEntropyLoss()
optimizer = optim.SGD(nnModel_cifar10.parameters(), lr = learning_Rate)


'''

Executing Training Loop with Neural Network Model

'''

training_Loop(
    n_epochs = epochs,
    optimizer = optimizer,
    model = nnModel_cifar10,
    loss_fn = loss_fn_cifar10,
    training_loader = training_loader
)